{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (__config__.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\IBM_ADMIN\\Anaconda3\\lib\\site-packages\\numpy\\__config__.py\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    lapack_opt_info={'libraries': ['mkl_core_dll', 'mkl_intel_lp64_dll', 'mkl_intel_thread_dll'], 'library_dirs': ['C:\\Users\\IBM_ADMIN\\Anaconda3\\\\Library\\\\lib'], 'define_macros': [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)], 'include_dirs': ['C:\\Users\\IBM_ADMIN\\Anaconda3\\\\Library\\\\include']}\u001b[0m\n\u001b[1;37m                                                                                                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Sample codes for predictive outcomes of knee replacement surgery replicating TKR work\n",
    "Any patient related data has been de-identified or removed in this example for the security purpose\n",
    "Method applied: missing data first imputed either by means or kmean clustering. GLM model, RF, and GBM tested\n",
    "\n",
    "\"\"\"\n",
    "######################################################\n",
    "###                import library                  ###\n",
    "######################################################\n",
    "# import library for feature selection, predictive model and dependencies for imputation\n",
    "import random\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from scipy import interp\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "# import library for cross validation, sensitivity and specificity\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "######################################################\n",
    "###                  Define functions              ###\n",
    "######################################################\n",
    "## function for k-mean imputation\n",
    "def km_impute(x_mis, centers):\n",
    "    ## function for imputing missing value in a panda vector using k_mean\n",
    "    # identify missing value index\n",
    "    delv = np.where(np.isnan(x_mis)==True)\n",
    "    keep = np.where(np.isnan(x_mis)==False)\n",
    "    # deleting the corresponding index in k mean centroid\n",
    "    kmc = np.delete(centers, delv, axis=1)\n",
    "    x_keep = x_mis.iloc[keep].values\n",
    "    # find the cluster closest to the complete vector (x_keep)\n",
    "    gp = metrics.pairwise_distances_argmin(x_keep, kmc)\n",
    "    # extract center and fill in the missing value    \n",
    "    gp_center = centers[gp]\n",
    "    x_mis.iloc[delv] = gp_center[0,delv]\n",
    "    return x_mis\n",
    "\n",
    "## function for getting ROC curve plot with 5-fold CV\n",
    "def ROC_CV(model, X, Y, outcome, cv):\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    for i in range(cv):    \n",
    "        # 5 fold cv scores\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y) \n",
    "        if np.sum(Y)/len(Y)<0.2:\n",
    "            # balancing sample        \n",
    "            Y_train.index = range(len(Y_train))\n",
    "            X_train.index = range(len(Y_train))        \n",
    "            k1 = Y_train.index[Y_train == 1]\n",
    "            k2 = random.sample(Y_train.index[Y_train ==0], len(k1))\n",
    "            k = list(k1) + list(k2)\n",
    "            Y_train = Y_train[k]\n",
    "            X_train = X_train.ix[k,:]        \n",
    "        # training model\n",
    "        probas_ = model.fit(X_train, Y_train).predict_proba(X_test)\n",
    "        fpr, tpr, threshold = metrics.roc_curve(Y_test, probas_[:,1])\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=1, label='ROC fold %d (area=%0.2f)' % (i, roc_auc))\n",
    "        \n",
    "    plt.plot([0,1], [0,1], '--', color = (0.6, 0.6, 0.6), label='50/50')\n",
    "    mean_tpr/=cv\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = metrics.auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, 'k--', label = 'Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic ' + outcome)\n",
    "    plt.legend(loc=\"lower right\", prop={'size':8})\n",
    "    plt.show()\n",
    "\n",
    "## function for obtaining feature importance value and graph\n",
    "def feature_importance(model, X, Y):\n",
    "    # feature importance\n",
    "    importances = model.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "    indices = np.argsort(importances)[::-1]    \n",
    "    # print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "    for f in range(X.shape[1]):\n",
    "        print(\"%d. feature %s (%f)\" % (f + 1, X.columns.values[indices[f]], importances[indices[f]]))    \n",
    "    # feature importnace graph\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    xlabel = X.columns.values[indices]\n",
    "    plt.bar(range(X.shape[1]), importances[indices], color=\"b\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(X.shape[1]), xlabel, rotation='vertical')\n",
    "    plt.xlim([-1, X.shape[1]])\n",
    "    plt.show()    \n",
    "\n",
    "## simple missing data imputation ##\n",
    "def impute(imp_method, X_orig, nc):\n",
    "    # option 1: simple mean \n",
    "    if (imp_method=='mean'):\n",
    "        print('imputing using mean')\n",
    "        imputer = Imputer(missing_values=np.nan, strategy=\"mean\", axis=0)\n",
    "        X_imp = pd.DataFrame(imputer.fit_transform(X_orig))\n",
    "        X_imp.columns = X_orig.columns\n",
    "        X_imp.index = X_orig.index\n",
    "    # option 2: K-mean\n",
    "    if(imp_method=='kmean'):\n",
    "        print('imputing using kmean')\n",
    "        # get k mean centers \n",
    "        k_means = KMeans(init='k-means++', n_clusters=nc, n_init=10)\n",
    "        X_slim_norm = (X_orig-X_orig.mean())/(X_orig.max() - X_orig.min())\n",
    "        k_means.fit(X_slim_norm.dropna(axis=0))\n",
    "        k_means_cluster_center = k_means.cluster_centers_\n",
    "        X_imp = X_slim_norm.transpose().apply(km_impute, args=(k_means_cluster_center,)).transpose()\n",
    "        # save group id\n",
    "        group_id = k_means.predict(X_imp)\n",
    "        # denormalize\n",
    "        X_imp = ((X_imp)*(X_orig.max() - X_orig.min()))+X_orig.mean()\n",
    "        # add group id\n",
    "        X_imp['group_id'] = group_id\n",
    "    return X_imp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "###        Import data, impute missing values      ###\n",
    "######################################################\n",
    "# import data from data mart\n",
    "knee_data=pd.read_csv('/data/new_feb_2017/processed/output/merged_data_year.csv', low_memory=False)\n",
    "\n",
    "# convert age category\n",
    "knee_data.ix[(knee_data['AGEC']=='>=80'),'AGEC'] = 80\n",
    "knee_data['AGEC']=pd.to_numeric(knee_data['AGEC'])\n",
    "knee_data['SEX'].replace(['F','M'], [1,0], inplace=True)\n",
    "\n",
    "\n",
    "# subset \n",
    "knee_data1 = knee_data.loc[(knee_data['DAYS'] >0) & (knee_data['DAYS'] <=365)]\n",
    "\n",
    "# set prediction target, change target as necessary:longlos, comp90, rev180, recov_issues, disp_home, recent_op, readmit30\n",
    "target = 'AE_site_1'\n",
    "# select comparable explorys features\n",
    "# Depression: GNMDYN7\n",
    "# Endocrine: GNMDYN11\n",
    "# Smoking:GNMDYN17\n",
    "# Anmeia: GNMDYN18 (hematological)\n",
    "# hypertenion: 'GNMDYN20'\n",
    "# Neuro: GNMDYN23\n",
    "# Rheumatism: GNMDYN21 (inflammatory disorder)\n",
    "# Respiratory: GNMDYN28\n",
    "\n",
    "dlist=['SEX', 'AGEC', 'BMI', 'GNMDYN7', 'GNMDYN11','GNMDYN17', 'GNMDYN18', 'GNMDYN20', 'GNMDYN21', 'GNMDYN23', 'GNMDYN28'] #hypertenion\n",
    "\n",
    "X_orig=knee_data1[dlist]\n",
    "# Impute to get complete data\n",
    "#X_imp=impute('kmean', X_orig, 9)\n",
    "X_imp = X_orig\n",
    "# Concat X & Y and remove row with missing Y\n",
    "#X_imp = X_imp.drop(['group_id'], axis=1)\n",
    "Y = knee_data1[target]\n",
    "df = pd.concat([X_imp, Y], axis=1)\n",
    "df = df.dropna()\n",
    "X = df.drop([target], axis=1)\n",
    "Y = df[target]\n",
    "\n",
    "######################################################\n",
    "### Feature selection based on classification tree ###\n",
    "######################################################\n",
    "# feature selection using tree-based method, previoius experiment tried L1 and chi-square\n",
    "clf = ExtraTreesClassifier()\n",
    "clf = clf.fit(X, Y)\n",
    "impt = clf.feature_importances_\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_new=pd.DataFrame(model.transform(X))\n",
    "c=np.array(X.columns.values)\n",
    "indices = np.argsort(impt)[::-1]\n",
    "od = sorted(indices[(range(X_new.shape[1]))])\n",
    "cname = c[od]\n",
    "X_new.columns=cname\n",
    "\n",
    "###################################################\n",
    "### Model accuracy and other evaluation metrics ###\n",
    "###################################################\n",
    "# specify predictive models\n",
    "glm = linear_model.LogisticRegression(C=1e5) \n",
    "rf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=1, random_state=0)\n",
    "gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "# define feature matrix and label\n",
    "X = X_new\n",
    "Y = Y\n",
    "# run GLM - logistic regression\n",
    "glm_fit = glm.fit(X, Y)\n",
    "# run random forest\n",
    "rf_fit = rf.fit(X, Y)\n",
    "# run gradient boosting\n",
    "gbm_fit = gbm.fit(X, Y)\n",
    "\n",
    "###################################################\n",
    "### Model accuracy and other evaluation metrics ###\n",
    "###################################################\n",
    "\n",
    "## ROC based on cross validation\n",
    "ROC_CV(glm_fit, X, Y, outcome=target, cv=5)\n",
    "ROC_CV(rf_fit, X, Y, outcome=target, cv=5)\n",
    "ROC_CV(gbm_fit, X, Y, outcome=target, cv=5)\n",
    "\n",
    "## feature importance from random forest\n",
    "feature_importance(rf_fit, X, Y)\n",
    "\n",
    "## sensitivity and specificty for each model\n",
    "# select predictive model\n",
    "mfit=gbm\n",
    "# begin calculation of sensitivity and specificity from cross validation\n",
    "cv = 5\n",
    "ppv = []\n",
    "sen = []\n",
    "for i in range(cv):    \n",
    "        # 5 fold cv scores\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y)\n",
    "        # further balacing the training set to have equal number of 0 nad 1\n",
    "        if np.sum(Y)/len(Y)<0.2:\n",
    "            Y_train.index = range(len(Y_train))\n",
    "            X_train.index = range(len(Y_train))        \n",
    "            k1 = Y_train.index[Y_train == 1]\n",
    "            k2 = random.sample(Y_train.index[Y_train ==0], len(k1))\n",
    "            k = list(k1) + list(k2)\n",
    "            Y_train = Y_train[k]\n",
    "            X_train = X_train.ix[k,:]\n",
    "        Y_pred = mfit.fit(X_train, Y_train).predict(X_test)\n",
    "        ppv.append(precision_score(Y_test, Y_pred))        \n",
    "        sen.append(recall_score(Y_test, Y_pred))\n",
    " \n",
    "np.mean(ppv)\n",
    "np.mean(sen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
